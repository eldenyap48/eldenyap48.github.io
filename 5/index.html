<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CS 180 Project 5 Part A: The Power of Diffusion Models!</title>
  <link href="https://fonts.googleapis.com/css2?family=Lato:wght@400;700&display=swap" rel="stylesheet">

  <style>
    body {
      font-family: 'Lato', sans-serif;
      text-align: center;
      margin: 0;
      padding: 0;
      background-color: #f9f9f9;
      color: #333;
    }

    .back-link {
      position: absolute;
      top: 20px;
      left: 20px;
      padding: 10px 18px;
      font-size: 1em;
      font-weight: 600;
      background-color: #eee;
      border-radius: 6px;
      text-decoration: none;
      color: #333;
      box-shadow: 0 2px 6px rgba(0,0,0,0.15);
      transition: background 0.2s ease, color 0.2s ease;
    }

    .back-link:hover {
      background-color: #ddd;
      color: #000;
    }

    h1 {
      font-size: 3em;
      margin: 60px 0 40px;
      font-weight: 700;
      color: #222;
    }

    .section {
      margin: 10px auto;
      padding: 10px 5px;
      width: 90%;
      background: #fff;
      border-radius: 12px;
      box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    }

    .section h3 {
      margin-bottom: 20px;
      font-size: 1.8em;
      font-weight: 600;
      color: #111;
    }

    .images {
      display: flex;
      justify-content: center;
      align-items: center;
      gap: 40px;
      margin-bottom: 20px;
    }

    img {
      max-width: 600px;
      max-height: 500px;
      border-radius: 10px;
      flex-shrink: 0;
      box-shadow: 0 3px 10px rgba(0,0,0,0.15);
      transition: transform 0.2s ease;
    }

    img:hover {
      transform: scale(1.05);
    }

    .description {
      font-size: 1.1em;
      line-height: 1.7;
      max-width: 1200px;
      margin: 30px auto;
      color: #555;
    }

    .big-img {
      max-width: 90%;
      height: auto;
      margin-top: 10px;
      margin-bottom: 10px;
    }
  </style>
</head>
<body>
  <a href="https://eldenyap48.github.io/" class="back-link">← Back</a>

  <h1>CS 180 Project 5 Part A: The Power of Diffusion Models!</h1>

  <div class="section">
    <h3>Part 0: Setup</h3>
    <div class="description">
        I designed a diverse set of creative text prompts and generated their corresponding prompt embeddings for use with the DeepFloyd IF text-to-image diffusion model. To ensure reproducibility across all experiments, I fixed the seed to 380 and consistently used it throughout the project. The following images were generated with the number of inference steps set to 20.
    </div>
    <div class="images">
      <img src="media/prompts_20_steps.png" class="big-img">
    </div>
    <div class="description">
        The following images were generated with the number of inference steps set to 40.
    </div>
    <div class="images">
      <img src="media/prompts_40_steps.png" class="big-img">
    </div>
    <div class="description">
        The generated images closely matched the semantic content of the text prompts, with clearer structure and finer details emerging as the number of inference steps increased. When using a lower number of steps, the outputs were faster to generate but appeared noisier and less coherent, while higher step counts produced sharper, more visually consistent results that better reflected the intended prompts.
    </div>

  <div class="section">
    <h3>Part 1.1: Implementing the Forward Process</h3>
    <div class="description">
      I implemented the forward (noising) process of a diffusion model, which progressively corrupts a clean image by adding Gaussian noise over time. Using a resized image of the Berkeley Campanile, I applied increasing noise levels at timesteps 250, 500, and 750 to visualize how structure is gradually destroyed as the diffusion process progresses.
    </div>
    <div class="images">
      <img src="media/noisy_campanile.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.2: Classical Denoising</h3>
    <div class="description">
      I applied classical Gaussian blur filtering to noisy versions of the Berkeley Campanile at timesteps 250, 500, and 750. The results show that while Gaussian blurring can reduce high-frequency noise at lower noise levels, it quickly fails to recover meaningful structure as noise increases, producing overly smooth and distorted images.
    </div>
    <div class="images">
      <img src="media/classical_denoisy.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.3: One-Step Denoising</h3>
    <div class="description">
      Using a pretrained diffusion UNet from DeepFloyd IF, I performed one-step denoising on noisy versions of the Campanile at timesteps 250, 500, and 750. The model successfully predicted the noise present in each image and enabled partial reconstruction of the original signal in a single reverse step. While fine details remain degraded at higher noise levels, the results clearly demonstrate the model’s ability to recover meaningful structure far beyond what classical denoising methods can achieve.
    </div>
    <div class="images">
      <img src="media/one_step_denoise.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.4: Iterative Denoising</h3>
    <div class="description">
      Using a strided diffusion schedule with an initial noise level of i_start = 10, I implemented the full iterative denoising process and visualized the progressive refinement of the Campanile image at regular intervals. The results clearly show how repeated diffusion steps gradually recover structure from heavy noise, producing a high-quality final reconstruction.
    </div>
    <div class="images">
      <img src="media/iterative_denoise_intermediates.png" class="big-img">
    </div>
    <div class="images">
      <img src="media/denoise_comparisons.png" class="big-img">
    </div>
    <div class="description">
      Both single-step denoising and Gaussian blur baselines perform significantly worse, highlighting the importance of multi-step diffusion for high-fidelity image restoration.
    </div>
  </div>

  <div class="section">
    <h3>Part 1.5: Diffusion Model Sampling</h3>
    <div class="description">
      By initializing the model with pure Gaussian noise and setting i_start = 0, I used the iterative diffusion process to generate images entirely from scratch. The model produces recognizable and semantically meaningful images from random noise, demonstrating the generative power of diffusion models.
    </div>
    <div class="images">
      <img src="media/diffusion_model_samples.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.6: Classifier-Free Guidance (CFG)</h3>
    <div class="description">
      I implemented classifier-free guidance (CFG) to significantly improve image quality by combining conditional and unconditional noise predictions during sampling. Using a guidance scale of γ = 7 with the prompt “a high quality photo”, the generated images became noticeably sharper, more coherent, and more semantically aligned with the text compared to unguided sampling. This demonstrates how CFG effectively trades off diversity for higher visual fidelity and stronger prompt adherence.
    </div>
    <div class="images">
      <img src="media/cfg_samples.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.7: Image-to-Image Translation</h3>
    <div class="description">
      Using classifier-free guidance, I applied image-to-image translation via the SDEdit process to progressively edit a real photograph by injecting controlled amounts of noise and denoising it back toward the data manifold. By varying the starting noise level, I generated a smooth spectrum of edits that transition from highly altered to nearly identical to the original image. This demonstrates how diffusion models can perform controllable, structure-preserving image edits rather than only generating images from pure noise.
    </div>
    <div class="images">
      <img src="media/sdedit_campanile.png" class="big-img">
      <img src="media/sdedit_ocean.png" class="big-img">
      <img src="media/sdedit_church.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.7.1: Editing Hand-Drawn and Web Images</h3>
    <div class="description">
      I applied the image-to-image diffusion editing process to both web-sourced photographs and hand-drawn sketches, demonstrating how non-photorealistic inputs can be projected onto the natural image manifold. By varying the noise level, I generated a progression of edits that transform abstract drawings and stylized images into increasingly realistic outputs. This highlights the model's ability to bridge the gap between human sketches and photorealistic imagery through controlled diffusion-based editing.
    </div>
    <div class="images">
      <img src="media/sdedit_labubu.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.7.2: Inpainting</h3>
    <div class="description">
      I implemented a diffusion-based inpainting pipeline that reconstructs missing regions of an image using a binary mask and classifier-free guidance. By constraining unmasked regions to remain faithful to the original image at every denoising step, the model realistically hallucinates new content only within the selected region. This method was applied to the Campanile and two custom examples, demonstrating precise, context-aware image completion driven by diffusion priors.
    </div>
    <div class="images">
      <img src="media/inpainted_campanile.png" class="big-img">
      <img src="media/inapinted_ocean.png" class="big-img">
      <img src="media/inpainted_church.png" class="big-img">
    </div>
  </div>

  <div class="section">
    <h3>Part 1.7.3: Text-Conditional Image-to-image Translation</h3>
    <div class="description">
    </div>
    <div class="images">
      <img src="media/inpainted_campanile.png" class="big-img">
      <img src="media/inapinted_ocean.png" class="big-img">
      <img src="media/inpainted_church.png" class="big-img">
    </div>
  </div>

</body>
</html>
